---
title: "Ch5 Resampling Methods"
author: "Kiril Raytchev"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(MASS)
library(tidymodels)
library(recipes)
library(tidyverse)
library(forcats)
library(GGally)
library(gridExtra)
library(grid)
library(scales)
library(cowplot)
library(rlang)
```

***

# Conceptual {#conceptual}

***

# *Ex.1* {#ex1} 

Using basic statistical properties of the variance, as well as single variable
calculus, derive (5.6). In other words, prove that $\alpha$ given by
(5.6) does indeed minimize Var($\alpha$X + (1 − $\alpha$)Y ).

>**Answer:** `r knitr::include_graphics("Ch5Ex1_1.jpg")`

# *Ex.2* {#ex2}

We will now derive the probability that a given observation is part
of a bootstrap sample. Suppose that we obtain a bootstrap sample
from a set of $n$ observations.

*(a)* What is the probability that the first bootstrap observation is
not the $j$th observation from the original sample? Justify your
answer.

>**Answer:** $1 - \frac{1}{n}$

*(b)* What is the probability that the second bootstrap observation
is not the $j$th observation from the original sample?

>**Answer:** Due to the fact that sampling is performed with replacement, we have the same answer as in *(a)* $1 - \frac{1}{n}$

*(c)* Argue that the probability that the $j$th observation is not in the
bootstrap sample is $(1 − 1/n)^n$.

>**Answer:** Since we deal with independent probabilities, the probability that the $j$th observation is not in the sample is the product of probabilities of each observation not being in the sample.

*(d)* When n = 5, what is the probability that the $j$th observation is
in the bootstrap sample?

>**Answer:** We use the procedure below to find:

```{r}
percent(1-(1-1/5)^5)
```

*(e)* When n = 100, what is the probability that the jth observation
is in the bootstrap sample?

>**Answer:** We use the procedure below to find:

```{r}
percent(1-(1-1/100)^100)
```

*(f)* When n = 10, 000, what is the probability that the jth observation
is in the bootstrap sample?

>**Answer:** We use the procedure below to find:

```{r}
percent(1-(1-1/10000)^10000)
```

*(g)* Create a plot that displays, for each integer value of n from 1
to 100, 000, the probability that the jth observation is in the
bootstrap sample. Comment on what you observe.

>**Answer:** We use the code below to create:

```{r}
x <- 1:100000
y <- 1-(1-1/x)^x
ggplot() +
        geom_point(aes(x, y))
```

Probability quickly reaches the values calculated in *(e)* and *(f)*.

*(h)* We will now investigate numerically the probability that a bootstrap
sample of size n = 100 contains the $j$th observation. Here
j = 4. We repeatedly create bootstrap samples, and each time
we record whether or not the fourth observation is contained in
the bootstrap sample.

 store=rep (NA , 10000)
 for (i in 1:10000) {
        store[i]=sum(sample (1:100 , rep =TRUE)==4) >0
 }
 mean(store)

Comment on the results obtained.

>**Answer:** We use the code below to obtain the results:

```{r}
 store=rep (NA , 10000)
 for (i in 1:10000) {
        store[i]=sum(sample (1:100 , rep =TRUE)==4) >0
 }
 mean(store)
```

which is close to what we calculate with formula in *(f)*: `r percent(1-(1-1/10000)^10000)`

# *Ex.3* {#ex3}

We now review k-fold cross-validation.

*(a)* Explain how k-fold cross-validation is implemented.

>**Answer:** This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method
is fit on the remaining k − 1 folds. (book_ref. 5.1.3)

*(b)* What are the advantages and disadvantages of k-fold cross validation
relative to:

*(i)* The validation set approach?
        
>**Answer:** The advantages are:   
1. Lower variability;   
2. More accurate test error estimates.  
The disadvantages are:   
1. Computational.

*(ii)* LOOCV?

>**Answer:** The advantages are:   
1. Computational;   
2. More accurate test error estimates (book_ref. 5.1.4).   
The disadvantages are:   
1. Higher variability.

# *Ex.4* {#ex4}

Suppose that we use some statistical learning method to make a prediction
for the response Y for a particular value of the predictor X.
Carefully describe how we might estimate the standard deviation of
our prediction.

>**Answer:** We can use the bootstrap approach. First, we predict *B-number of times* with replacement **Y** for particular values of **X**. Second, we compute the SE of the estimates by using formula (5.8)

***

# Applied {#applied}

***

```{r}
rm(list = ls())
```

# *Ex.5* {#ex5}

In Chapter 4, we used logistic regression to predict the probability of
default using `income` and `balance` on the `Default` data set. We will
now estimate the test error of this logistic regression model using the
validation set approach. Do not forget to set a random seed before
beginning your analysis.

*(a)* Fit a logistic regression model that uses `income` and `balance` to
predict `default`.

>**Answer:** ....
 


